# @file config/config.yaml
# @description Configuration for the Cablegate PDF Dataset Project
# @version 0.0.3
# @license MIT

dataset:
  name: "cablegate-pdf-dataset"
  type: "parquet"
paths:
  root: "."
  assets: "./assets"
  data: "./data"
  raw: "./data/raw"
  process: "./data/process"
  database: "./data/db"
  export: "./data/export"
  generate: "./data/generate"
  processed_data: "processed_data"
  processed_data_train: "{{paths.processed_data}}/train"
  templates: "{{paths.assets}}/templates"
files:
  include:
  - "train-00000-of-00001.{{dataset.type}}"
  exclude:
  - "cleaned_data.{{dataset.type}}"
  load:
  - "{{paths.process}}/{file_name}"
database:
  env: "development"
  name: "thinking-dataset"
  type: "sqlite"
  url: "{{database.type}}:///data/db/{{database.name}}.db"
  config:
    pool_size: 5
    max_overflow: 10
    connect_timeout: 30
    read_timeout: 30
    log_queries: True
providers:
- provider:
    name: "localhost"
    type: "ollama"
    url: "http://localhost:11434"
    config:
      model: "llama3.1:latest"
      stream: False
      options:
        num_ctx: 64768
        temperature: 1.6
        repeat_penalty: 1.2
        num_thread: 64
pipelines:
- pipeline:
    name: "process"
    description: "Default prepare pipeline"
    config:
      prepare_file: "{file_base}{file_ext}"
    pipes:
    - pipe:
        type: "SubsetPipe"
        config:
          rows: [ 0, 100 ]
          columns: [ "all" ]
    - pipe:
        type: "AddIdPipe"
        config: {}
    - pipe:
        type: "DropColumnsPipe"
        config:
          columns: [ "file_name" ]
    - pipe:
        type: "RemapColumnsPipe"
        config:
          column_mapping:
            pdf_content: cable
          column_order:
          - id
          - cable
    - pipe:
        type: "RemoveDuplicatesPipe"
        config:
          columns: [ "auto" ]
    - pipe:
        type: "HandleMissingValuesPipe"
        config:
          columns: [ "auto" ]
          remove_partials: True
          allow_empty: False
    - pipe:
        type: "NormalizeTextPipe"
        config:
          columns: [ "cable" ]
          contractions:
            "ain't": "am not"
          terms:
            "i.e": "that is "
    - pipe:
        type: "FilterBySizePipe"
        config:
          column_name: "cable"
          min_size: 500
          max_size: 0
    - pipe:
        type: "ChunkingPipe"
        config:
          columns: [ "cable" ]
          min_chunk_size: 500
          max_chunk_size: 2000
- pipeline:
    name: "generate"
    description: "Generate synthetic data pipeline"
    config:
      batch_size: 1
    pipes:
    - pipe:
        type: "QueryGenerationPipe"
        config:
          flush: True
          template: "{{paths.templates}}/thinking.md"
          input:
          - source:
              table: "train-00000-of-00001"
              column: "cable"
              label: "seed"
              amount: 3
              length: 2000
              offset: 0
              ellipsis: "..."
              shuffle: True
          output:
          - source:
              table: "cables"
              column: "query"
              if_exists: "replace"
    - pipe:
        type: "ResponseGenerationPipe"
        config:
          provider: "localhost"
          if_exists: "replace"
          max_workers: 1
          mock: False
          min_length: 3000
          template: "{{paths.templates}}/thinking.md"
          output:
          - table: "cables"
            columns: [ "thinking" ]
- pipeline:
    name: "export"
    description: "Default export pipeline"
    config: {}
    pipes:
    - pipe:
        type: "ExportTablesPipe"
        config:
          tables: [ "cables" ]
          columns: [ "auto" ]
          shard_size: 1000000
          pattern: "{{dataset.name}}-{split_name}-{split_info}.{{dataset.type}}"
          path: "{{paths.export}}/{{paths.processed_data_train}}"
          schema: [ "id", "query", "seeds", "thinking", "reasoning", "reflecting", "composing", "evaluation" ]
          drop_columns: True
          fill_value: "null"
- pipeline:
    name: "upload"
    description: "Default upload pipeline"
    config: {}
    pipes:
    - pipe:
        type: "FileExtractorPipe"
        config:
          path: "{{paths.export}}/{{paths.processed_data_train}}"
          filter: "{{dataset.type}}"
    - pipe:
        type: "FileUploadHfApiPipe"
        config:
          org: "${HF_ORG}"
          user: "${HF_USER}"
          token: "${HF_WRITE_TOKEN}"
          dataset: "{{dataset.name}}"
          repo_type: "dataset"
          remote_path: "{{paths.processed_data_train}}"
          dry_run: False
