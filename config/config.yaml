# @file config/config.yaml
# @description Configuration for the Cablegate PDF Dataset Project
# @version 1.0.4
# @license MIT

dataset:
  name: "cablegate-pdf-dataset"
  type: "parquet"
paths:
  root: "."
  data: "./data"
  raw: "./data/raw"
  process: "./data/process"
  database: "./data/db"
  export: "./data/export"
  processed_data: "processed_data"
  processed_data_train: "{{paths.export}}/{{paths.processed_data}}/train"
database:
  env: "development"
  name: "thinking-dataset"
  type: "sqlite"
  url: "{{database.type}}:///data/db/{{database.name}}.db"
  config:
    pool_size: 5
    max_overflow: 10
    connect_timeout: 30
    read_timeout: 30
    log_queries: True
files:
  include:
  - "train-00000-of-00001.{{dataset.type}}"
  exclude:
  - "cleaned_data.{{dataset.type}}"
  load:
  - "{{paths.process}}/{file_name}"
pipelines:
- pipeline:
    name: "prepare"
    description: "Default prepare pipeline"
    config:
      prepare_file: "{file_base}{file_ext}"
    pipes:
    - pipe:
        type: "SubsetPipe"
        config:
          rows: [ 0, 100 ]
          columns: [ "all" ]
    - pipe:
        type: "AddIdPipe"
        config: {}
    - pipe:
        type: "DropColumnsPipe"
        config:
          columns: [ "file_name" ]
    - pipe:
        type: "RemapColumnsPipe"
        config:
          column_mapping:
            pdf_content: cable
          column_order:
          - id
          - cable
    - pipe:
        type: "RemoveDuplicatesPipe"
        config:
          columns: [ "auto" ]
    - pipe:
        type: "HandleMissingValuesPipe"
        config:
          columns: [ "auto" ]
          remove_partials: True
          allow_empty: False
    - pipe:
        type: "NormalizeTextPipe"
        config:
          columns: [ "cable" ]
          contractions:
            "ain't": "am not"
          terms:
            "i.e": "that is "
    - pipe:
        type: "FilterBySizePipe"
        config:
          column_name: "cable"
          min_size: 500
          max_size: 0
    - pipe:
        type: "ChunkingPipe"
        config:
          columns: [ "cable" ]
          min_chunk_size: 500
          max_chunk_size: 10000
- pipeline:
    name: "export"
    description: "Default export pipeline"
    config: {}
    pipes:
    - pipe:
        type: "ExportTablesPipe"
        config:
          tables: [ "all" ]
          columns: [ "auto" ]
          shard_size: 1000000
          pattern: "{{dataset.name}}-{split_name}-{split_info}.{{dataset.type}}"
          path: "{{paths.processed_data_train}}"
          schema: [ "id", "cable" ]
          drop_columns: True
          fill_value: "null"
- pipeline:
    name: "upload"
    description: "Default upload pipeline"
    config: {}
    pipes:
    - pipe:
        type: "FileExtractorPipe"
        config:
          path: "{{paths.processed_data_train}}"
          filter: "{{dataset.type}}"
    - pipe:
        type: "FileUploadHfApiPipe"
        config:
          org: "${HF_ORG}"
          dataset: "{{dataset.name}}"
          token: "${HF_WRITE_TOKEN}"
          user: "${HF_USER}"
          repo_type: "dataset"
          out_path: "{{paths.processed_data}}"
          dry_run: True
